{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network to Recognise Hand Written Digits Using Keras\n",
    "\n",
    "#### In this session, we will be using keras, a top level wrapper for Tensorflow to build Neural Networks that can classify hand-written digits.\n",
    "\n",
    "## We will be building three types of networks :\n",
    "* Feed-Forward Neural Network: This is a simple neural network with only dense connections\n",
    "* Convolutional Neural Network: This is a more advanced network\n",
    "* Transfer Learning: We will use a network trained on the ImageNet data and use it to classify MNIST data\n",
    "\n",
    "## You will need :\n",
    "* keras \n",
    "* matplotlib - Used for data visualisation\n",
    "* numpy - Used for Matrix Operations\n",
    "\n",
    "# Let's Get Started!\n",
    "\n",
    "## Feed-Forward Neural Network\n",
    "\n",
    "### Step 1: First we have to import all the packages that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras                         # Used for building the model and getting the MNIST data\n",
    "from keras.datasets import mnist     # Used for downloading and using MNIST data\n",
    "from keras.models import Sequential  # For building the model. Case Sensitive! 'S' is Capital!\n",
    "from keras.layers import Dense       # For fully connected layer\n",
    "from keras.optimizers import Adam    # Optimiser used for training\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt      # For visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Now we have to set up some Hyper Parameters\n",
    "\n",
    "Hyper-parameters are certain constants in the neural network.\n",
    "These include the learning rate, number of layers and number of neurons in each layer.\n",
    "It is hard to know what the 'ideal' values of these hyper-parameters are, so we use certain rules-of-thumb and trial and error to select our hyper-parameters.\n",
    "They can be adjusted to get different results from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We feed our data into the network in batches.\n",
    "# We chose a batch size of 128 meaning each batch contains 128 images.\n",
    "batch_size = 128   \n",
    "# There are 10 digits, so we have 10 classes\n",
    "num_classes = 10\n",
    "# We will train our network for 5 epochs\n",
    "epochs = 5\n",
    "\n",
    "# MNIST images are of size 28x28 pixels\n",
    "#These values will help us in plotting images\n",
    "img_size = 28\n",
    "img_shape = (img_size, img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a Function to plot the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Download, Plot and Preprocess Images\n",
    "\n",
    "The `load_data()` function is used to download the data.\n",
    "\n",
    "It returns four numpy arrays containing both the training and the test data.\n",
    "\n",
    "The 'x' values contain the images as a vector. Each image is 28x28 pixels in size, meaning that when unfolded into a vector, we get 784 values.\n",
    "\n",
    "There are 60,000 train images and 10,000 test images.\n",
    "\n",
    "The 'y' values are one-hot encoded vectors which gives us the corresponding class of the image in the 'x' arrays.\n",
    "\n",
    "After downloading the images, we have to reshape the numpy array into the share that we want it to be in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the data in the form we want.\n",
    "# This will create 60,000 rows with each row containing 784 columns, each containing a pixel value\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# Convert the images into type float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Let's plot some of the images\n",
    "plot_images(x_train[0:9])\n",
    "\n",
    "# We normalise the images as it helps in training\n",
    "# This means that we convert the pixel values (which are in the range 0-255) to a range (0-1).\n",
    "# This is done by dividing it by 255\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Let's see how many train and testing samples we have\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Now we convert the classes into One-Hot Encoded Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Building the Neural Network!!!\n",
    "\n",
    "The Model is built using the Sequential Function\n",
    "\n",
    "We add layers to the model step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Training the model\n",
    "\n",
    "Now that we have our model built, we can start to train our model. This is done using the compile() function.\n",
    "\n",
    "We use a categorical crossentropy loss function.\n",
    "The optimiser we use is Adam.\n",
    "And the metric that the neural network tries to optimise is the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.2166 - acc: 0.9355 - val_loss: 0.0980 - val_acc: 0.9712\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0787 - acc: 0.9759 - val_loss: 0.0790 - val_acc: 0.9745\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0511 - acc: 0.9836 - val_loss: 0.0664 - val_acc: 0.9795\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0352 - acc: 0.9884 - val_loss: 0.0728 - val_acc: 0.9776\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0280 - acc: 0.9909 - val_loss: 0.0720 - val_acc: 0.9768\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Now we can see our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9768\n"
     ]
    }
   ],
   "source": [
    "## print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an appreciable accuracy of 97%+.\n",
    "Let's see if we can improve it by using a convolutional nerual network\n",
    "\n",
    "## Convolutional Neural Network\n",
    "\n",
    "Now we will start to train a convolutional neural network.\n",
    "\n",
    "### Step 9: Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten   # This is used to get a vector of all the filter outputs\n",
    "from keras.layers import Conv2D    # This is the Conv2D filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Reshaping the Training Data\n",
    "\n",
    "A Conv2D layer takes as input an image of shape:\n",
    "No. of Rows x No. of Columns x No. of Channels\n",
    "\n",
    "In our case, the number of channels is 1. In case of color images like the CIFAR-10 dataset, the number of channels will be 3 (Red, Green, Blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_size, img_size, channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_size, img_size, channels)\n",
    "input_shape = (img_size, img_size, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 206s 3ms/step - loss: 0.1840 - acc: 0.9427 - val_loss: 0.0590 - val_acc: 0.9805\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 205s 3ms/step - loss: 0.0477 - acc: 0.9852 - val_loss: 0.0412 - val_acc: 0.9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1338b39b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Evaluating our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04115520673741121\n",
      "Test accuracy: 0.9864\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
