{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wwc_meetup.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "AqjKM4m36oyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Network to Recognise Hand Written Digits Using Keras\n",
        "\n",
        "#### In this session, we will be using keras, a top level wrapper for Tensorflow to build Neural Networks that can classify hand-written digits.\n",
        "\n",
        "## We will be building three types of networks :\n",
        "* Feed-Forward Neural Network: This is a simple neural network with only dense connections\n",
        "* Convolutional Neural Network: This is a more advanced network\n",
        "\n",
        "## You will need :\n",
        "* keras \n",
        "* matplotlib - Used for data visualisation\n",
        "* numpy - Used for Matrix Operations\n",
        "\n",
        "# Let's Get Started!\n",
        "\n",
        "## Feed-Forward Neural Network\n",
        "\n",
        "### Step 1: First we have to import all the packages that we need"
      ]
    },
    {
      "metadata": {
        "id": "-lacTw8e6oyb",
        "colab_type": "code",
        "outputId": "a7710ff7-98f1-40cc-eec2-ec18d136dcea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras                         # Used for building the model and getting the MNIST data\n",
        "from keras.datasets import mnist     # Used for downloading and using MNIST data\n",
        "from keras.models import Sequential  # For building the model. Case Sensitive! 'S' is Capital!\n",
        "from keras.layers import Dense       # For fully connected layer\n",
        "from keras.optimizers import Adam    # Optimiser used for training\n",
        "\n",
        "import numpy as np                   # Used for matrix operations\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt      # For visualisations"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HZQITZ5E6oyj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2: Now we have to set up some Hyper Parameters\n",
        "\n",
        "Hyper-parameters are certain constants in the neural network.\n",
        "These include the learning rate, number of layers and number of neurons in each layer.\n",
        "It is hard to know what the 'ideal' values of these hyper-parameters are, so we use certain rules-of-thumb and trial and error to select our hyper-parameters.\n",
        "They can be adjusted to get different results from the neural network."
      ]
    },
    {
      "metadata": {
        "id": "wpZ9rcyU6oyl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We feed our data into the network in batches.\n",
        "# We chose a batch size of 128 meaning each batch contains 128 images.\n",
        "batch_size = 128   \n",
        "# There are 10 digits, so we have 10 classes\n",
        "num_classes = 10\n",
        "# We will train our network for 5 epochs\n",
        "epochs = 5\n",
        "\n",
        "# MNIST images are of size 28x28 pixels\n",
        "#These values will help us in plotting images\n",
        "img_size = 28\n",
        "img_shape = (img_size, img_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MgpORTgi6oys",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create a Function to plot the images"
      ]
    },
    {
      "metadata": {
        "id": "_WhgxacV6oyu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_images(img):\n",
        "    \n",
        "    img=img.reshape(img_shape)\n",
        "    plt.imshow(img, cmap='binary')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vFt_ZO5_6oyy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 4: Download, Plot and Preprocess Images\n",
        "\n",
        "The `load_data()` function is used to download the data.\n",
        "\n",
        "It returns four numpy arrays containing both the training and the test data.\n",
        "\n",
        "The 'x' values contain the images as a vector. Each image is 28x28 pixels in size, meaning that when unfolded into a vector, we get 784 values.\n",
        "\n",
        "There are 60,000 train images and 10,000 test images.\n",
        "\n",
        "The 'y' values are one-hot encoded vectors which gives us the corresponding class of the image in the 'x' arrays.\n",
        "\n",
        "After downloading the images, we have to reshape the numpy array into the share that we want it to be in."
      ]
    },
    {
      "metadata": {
        "id": "ux8LA0kY6oy0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ciy6kwlR6oy6",
        "colab_type": "code",
        "outputId": "76775ef2-5609-4012-e92a-1c9aee2769ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "cell_type": "code",
      "source": [
        "# Reshaping the data in the form we want.\n",
        "# This will create 60,000 rows with each row containing 784 columns, each containing a pixel value\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "# Convert the images into type float32\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Let's plot some of the images\n",
        "plot_images(x_train[2])\n",
        "\n",
        "\n",
        "# We normalise the images as it helps in training\n",
        "# This means that we convert the pixel values (which are in the range 0-255) to a range (0-1).\n",
        "# This is done by dividing it by 255\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Let's see how many train and testing samples we have\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jcjoyYbj6ozG",
        "colab_type": "code",
        "outputId": "bb7dfe16-07d9-492d-bc82-9163961eac87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "print('y_train format before one hot encoding:')\n",
        "print(y_train[0:5])\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "\n",
        "print('y_train format after one hot encoding:')\n",
        "print(y_train[0:5])\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train format before one hot encoding:\n",
            "[5 0 4 1 9]\n",
            "y_train format after one hot encoding:\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vWBC5nfv6ozE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 5: Now we convert the classes into One-Hot Encoded Vectors"
      ]
    },
    {
      "metadata": {
        "id": "ICaPsfP46ozJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 6: Building the Neural Network!!!\n",
        "\n",
        "The Model is built using the Sequential Function\n",
        "\n",
        "We add layers to the model step by step."
      ]
    },
    {
      "metadata": {
        "id": "Ic3k7Z_E6ozK",
        "colab_type": "code",
        "outputId": "8abcca0d-dbd1-4f05-87e6-2ca15ea56492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tCow4-5p6ozO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 7: Training the model\n",
        "\n",
        "Now that we have our model built, we can start to train our model. This is done using the compile() function.\n",
        "\n",
        "We use a categorical crossentropy loss function.\n",
        "The optimiser we use is Adam.\n",
        "And the metric that the neural network tries to optimise is the accuracy."
      ]
    },
    {
      "metadata": {
        "id": "Bq9K1lkm6ozQ",
        "colab_type": "code",
        "outputId": "e6ff68fd-ac31-4dec-c522-49e5fefc1a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        }
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 10s 163us/step - loss: 0.2156 - acc: 0.9374 - val_loss: 0.0995 - val_acc: 0.9683\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 10s 160us/step - loss: 0.0788 - acc: 0.9762 - val_loss: 0.0858 - val_acc: 0.9722\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 10s 161us/step - loss: 0.0493 - acc: 0.9843 - val_loss: 0.0675 - val_acc: 0.9783\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0350 - acc: 0.9886 - val_loss: 0.0664 - val_acc: 0.9816\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 10s 164us/step - loss: 0.0276 - acc: 0.9906 - val_loss: 0.0912 - val_acc: 0.9740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JrMPhzx66ozW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 8: Now we can see our results!"
      ]
    },
    {
      "metadata": {
        "id": "PyLoOUe_6ozX",
        "colab_type": "code",
        "outputId": "1f55e9c9-daf2-4ffd-a826-5b51c62f0952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "## print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "27IzEx6zOF6w",
        "colab_type": "code",
        "outputId": "87e2d899-0925-4124-d3fa-d7a27f331b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "cell_type": "code",
      "source": [
        "plot_images(x_test[6])\n",
        "# print(x_test[6])\n",
        "# print(x_test[6].shape)\n",
        "\n",
        "prediction=model.predict(x_test[6].reshape(1, 784)) # Need to reshape here\n",
        "\n",
        "print('Prediction is: ')\n",
        "print(np.argmax(prediction)) # Getting the position of the max value in the prediction list"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJWDQpQQMupSAQZcSMOhSAv8HxJG6h8NyAc8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Prediction is: \n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "29UUjdJM6ozb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We got an appreciable accuracy of 97%+.\n",
        "Let's see if we can improve it by using a convolutional nerual network\n",
        "\n",
        "## Convolutional Neural Network\n",
        "\n",
        "Now we will start to train a convolutional neural network.\n",
        "\n",
        "### Step 9: Importing Modules"
      ]
    },
    {
      "metadata": {
        "id": "PRUj9zmc6ozd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten   # This is used to get a vector of all the filter outputs\n",
        "from keras.layers import Conv2D    # This is the Conv2D filter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NlnX8oQ36ozg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 10: Reshaping the Training Data\n",
        "\n",
        "A Conv2D layer takes as input an image of shape:\n",
        "No. of Rows x No. of Columns x No. of Channels\n",
        "\n",
        "In our case, the number of channels is 1. In case of color images like the CIFAR-10 dataset, the number of channels will be 3 (Red, Green, Blue)"
      ]
    },
    {
      "metadata": {
        "id": "jCwCvpX_6ozh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "channels = 1\n",
        "\n",
        "x_train_cnn = x_train.reshape(x_train.shape[0], img_size, img_size, channels)\n",
        "x_test_cnn = x_test.reshape(x_test.shape[0], img_size, img_size, channels)\n",
        "input_shape_cnn = (img_size, img_size, channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UtYULQOo6ozj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 11: Building the model"
      ]
    },
    {
      "metadata": {
        "id": "H0ZXTrbM6ozk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8b53131c-208f-42ab-d43e-c8c56f8325da"
      },
      "cell_type": "code",
      "source": [
        "model_cnn = Sequential()\n",
        "model_cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape_cnn))\n",
        "model_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(128, activation='relu'))\n",
        "model_cnn.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model_cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model_cnn.fit(x_train_cnn, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=2,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test_cnn, y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "60000/60000 [==============================] - 211s 4ms/step - loss: 0.1942 - acc: 0.9399 - val_loss: 0.0505 - val_acc: 0.9840\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 210s 4ms/step - loss: 0.0484 - acc: 0.9850 - val_loss: 0.0426 - val_acc: 0.9861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-TLtqx3f6ozn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 12: Evaluating our model performance"
      ]
    },
    {
      "metadata": {
        "id": "Bg-11_Yd6ozp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79fd7484-5181-47df-ed67-9112ada5aa63"
      },
      "cell_type": "code",
      "source": [
        "score = model_cnn.evaluate(x_test_cnn, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.04259198339713039\n",
            "Test accuracy: 0.9861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cpnChH12-H4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "f7ecf5e3-6589-4575-c1f4-bf6e537c444b"
      },
      "cell_type": "code",
      "source": [
        "plot_images(x_test[10])\n",
        "\n",
        "prediction_cnn=model_cnn.predict(x_test[10].reshape(1, 28, 28, 1))\n",
        "\n",
        "print('The prediction is: ')\n",
        "print(np.argmax(prediction_cnn))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The prediction is: \n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}